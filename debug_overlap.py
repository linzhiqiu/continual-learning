# For label verification
import sys
sys.path.append("./CLIP")
import os
import time
import shutil

from pathlib import Path
import argparse
from utils import load_json, save_as_json

argparser = argparse.ArgumentParser()
argparser.add_argument("--public_folder",
                       default="/data3/zhiqiul/CLEAR-10-PUBLIC/", type=str,
                       help="Public folder generated by prepare_concepts.py")
argparser.add_argument("--original_folder",
                       default="/data3/zhiqiul/clear_datasets/CLEAR10-TEST/", type=str,
                       help="Original folder generated by prepare_concepts.py")
argparser.add_argument("--new_folder",
                       default="/data3/zhiqiul/clear_datasets/CLEAR10-TEST-CLEANED/", type=str,
                       help="New folder to save")
argparser.add_argument("--cleaned_folder",
                       default="/data3/zhiqiul/clear_datasets/CMU_1_json_20220111-all", type=str,
                       help="Folder with feedback")
argparser.add_argument("--label_mapper",
                       default="/data3/zhiqiul/clear_datasets/CMU_1_json_20220111-all/label_map.json", type=str,
                       help="Map original labels to user-defined labels")
argparser.add_argument("--num_imgs",
                       default=150, type=int,
                       help="Number of images to save in clean folder")

def retrieve_cleaned_folder_labels(cleaned_folder, bucket_indices):
    user_label_set = set()
    for i, b_idx in enumerate(bucket_indices):
        cleaned_folder_i = cleaned_folder / b_idx
        assert cleaned_folder_i.exists()
        cleaned_folder_i_labels = list(cleaned_folder_i.glob('[!.]*/'))
        if i == 0:
            sorted_prompts = sorted([str(f)[len(str(cleaned_folder_i))+1:] for f in cleaned_folder_i_labels])
        else:
            new_sorted_prompts = sorted([str(f)[len(str(cleaned_folder_i))+1:] for f in cleaned_folder_i_labels])
            if not sorted_prompts == new_sorted_prompts:
                import pdb; pdb.set_trace()
        for label in sorted_prompts:
            cleaned_folder_i_label = cleaned_folder_i / label
            json_files = cleaned_folder_i_label.glob('*.json')
            for json_file in json_files:
                res = load_json(json_file)
                user_labels = res['task_result']['annotations'][0]['input']['value']
                for user_label in user_labels:
                    user_label_set.add(user_label)
    return list(user_label_set)

def retrieve_cleaned_folder_information(cleaned_folder, bucket_indices, label_mapper_dict):
    cleaned_dict = {
        'sorted_prompts' : [],
        'cleaned_images' : {},
    }
    for i, b_idx in enumerate(bucket_indices):
        cleaned_folder_i = cleaned_folder / b_idx
        assert cleaned_folder_i.exists()
        cleaned_folder_i_labels = list(cleaned_folder_i.glob('[!.]*/'))
        if i == 0:
            cleaned_dict['sorted_prompts'] = sorted([str(f)[len(str(cleaned_folder_i))+1:] for f in cleaned_folder_i_labels])
        else:
            assert cleaned_dict['sorted_prompts'] == sorted([str(f)[len(str(cleaned_folder_i))+1:] for f in cleaned_folder_i_labels])
        
        cleaned_dict['cleaned_images'][b_idx] = {}
        for label in cleaned_dict['sorted_prompts']:
            cleaned_dict['cleaned_images'][b_idx][label] = [] # list of flickr ID
            cleaned_folder_i_label = cleaned_folder_i / label
            json_files = cleaned_folder_i_label.glob('*.json')
            for json_file in json_files:
                ID = str(json_file)[len(str(cleaned_folder_i_label))+1:-5]
                res = load_json(json_file)
                user_labels = [label_mapper_dict[k] for k in res['task_result']['annotations'][0]['input']['value']]
                if len(user_labels) == 1 and user_labels[0] == label:
                    # if len(cleaned_dict['cleaned_images'][b_idx][label]) < num_imgs:
                    cleaned_dict['cleaned_images'][b_idx][label].append(ID)
            # if len(cleaned_dict['cleaned_images'][b_idx][label]) < num_imgs:
            #     print(f"Not enough images for bucket {b_idx} label {label}")
            #     import pdb; pdb.set_trace()
    return cleaned_dict           

if __name__ == '__main__':
    args = argparser.parse_args()

    # public_bucket_dict = load_json("/scratch/zhiqiu/yfcc100m_all/images_minbyte_10_valid_uploaded_date_feb_18/bucket_11.json")
    
    # print(f"For public folder:")
    # for b_idx in public_bucket_dict:
    #     min_line_num_str = None
    #     min_line_num_int = None
    #     max_line_num_str = None
    #     max_line_num_int = None
    #     min_date = public_bucket_dict[b_idx]['min_date']
    #     max_date = public_bucket_dict[b_idx]['max_date']
    #     all_metadata = public_bucket_dict[b_idx]['all_metadata']
    #     for meta in all_metadata:
    #         line_num_int = int(meta['LINE_NUM'])
    #         line_num_str = meta['LINE_NUM']
    #         if min_line_num_int == None or min_line_num_str == None:
    #             min_line_num_int = line_num_int
    #             min_line_num_str = line_num_str
    #         if max_line_num_int == None or max_line_num_str == None:
    #             max_line_num_int = line_num_int
    #             max_line_num_str = line_num_str
    #         if min_line_num_int > line_num_int:
    #             min_line_num_int = line_num_int
    #         if min_line_num_str > line_num_str:
    #             min_line_num_str = line_num_str
    #         if max_line_num_int < line_num_int:
    #             max_line_num_int = line_num_int
    #         if max_line_num_str < line_num_str:
    #             max_line_num_str = line_num_str
        
    #     print(f"Bucket {b_idx}: {min_date} to {max_date}. ")
    #     print(f"Min line num: Int={min_line_num_int}, Str={min_line_num_str}.")
    #     print(f"Max line num: Int={max_line_num_int}, Str={max_line_num_str}.")
    # For public folder:
    # Bucket 0: 2004-01-01 11:52:13 to 2007-04-09 03:27:17.
    # Min line num: Int=21, Str=10000009.
    # Max line num: Int=10052716, Str=999999.
    # Bucket 1: 2007-04-09 03:27:33 to 2008-03-21 19:14:24.
    # Min line num: Int=2, Str=1000.
    # Max line num: Int=10052674, Str=9999993.
    # Bucket 2: 2008-03-21 19:14:37 to 2008-12-23 15:03:12.
    # Min line num: Int=12, Str=10000021.
    # Max line num: Int=10052681, Str=9999975.
    # Bucket 3: 2008-12-23 15:03:36 to 2009-09-07 09:09:32.
    # Min line num: Int=9, Str=10000014.
    # Max line num: Int=10052717, Str=9999988.
    # Bucket 4: 2009-09-07 09:09:43 to 2010-06-03 01:36:09.
    # Min line num: Int=4, Str=10.
    # Max line num: Int=10052698, Str=9999962.
    # Bucket 5: 2010-06-03 01:36:29 to 2011-02-05 02:18:50.
    # Min line num: Int=15, Str=10000011.
    # Max line num: Int=10052710, Str=9999998.
    # Bucket 6: 2011-02-05 02:19:02 to 2011-09-24 10:04:39.
    # Min line num: Int=27, Str=10000028.
    # Max line num: Int=10052706, Str=999998.
    # Bucket 7: 2011-09-24 10:06:13 to 2012-06-05 04:44:33.
    # Min line num: Int=3, Str=1000000.
    # Max line num: Int=10052712, Str=9999989.
    # Bucket 8: 2012-06-05 04:44:46 to 2013-02-03 03:34:43.
    # Min line num: Int=38, Str=10000000.
    # Max line num: Int=10052697, Str=9999999.
    # Bucket 9: 2013-02-03 03:34:45 to 2013-09-08 22:35:14.
    # Min line num: Int=5, Str=10000.
    # Max line num: Int=10052687, Str=9999996.
    # Bucket 10: 2013-09-08 22:35:26 to 2014-04-28 10:49:19.
    # Min line num: Int=1, Str=1.
    # Max line num: Int=10052713, Str=9999987.

    # Bucket 0: 2004-01-01 11:52:13 to 2007-04-09 03:27:17.
    # Min line num: Int=21, Str=10000009.
    # Max line num: Int=52949999, Str=999999.
    # Bucket 1: 2007-04-09 03:27:18 to 2008-03-21 19:14:19.
    # Min line num: Int=2, Str=1000.
    # Max line num: Int=52949991, Str=9999993.
    # Bucket 2: 2008-03-21 19:14:25 to 2008-12-23 15:03:12.
    # Min line num: Int=12, Str=10000021.
    # Max line num: Int=52949958, Str=9999975.
    # Bucket 3: 2008-12-23 15:03:14 to 2009-09-07 09:09:32.
    # Min line num: Int=9, Str=10000014.
    # Max line num: Int=52949974, Str=9999988.
    # Bucket 4: 2009-09-07 09:09:35 to 2010-06-03 01:36:09.
    # Min line num: Int=4, Str=10.
    # Max line num: Int=52949988, Str=9999962.
    # Bucket 5: 2010-06-03 01:36:29 to 2011-02-05 02:18:50.
    # Min line num: Int=15, Str=10000011.
    # Max line num: Int=52949997, Str=9999998.
    # Bucket 6: 2011-02-05 02:18:51 to 2011-09-24 10:04:39.
    # Min line num: Int=27, Str=10000028.
    # Max line num: Int=52949980, Str=999998.
    # Bucket 7: 2011-09-24 10:04:45 to 2012-06-05 04:44:33.
    # Min line num: Int=3, Str=1000000.
    # Max line num: Int=52949976, Str=9999989.
    # Bucket 8: 2012-06-05 04:44:37 to 2013-02-03 03:34:43.
    # Min line num: Int=38, Str=10000000.
    # Max line num: Int=52949995, Str=9999999.
    # Bucket 9: 2013-02-03 03:34:45 to 2013-09-08 22:35:14.
    # Min line num: Int=5, Str=10000.
    # Max line num: Int=52949994, Str=9999996.
    # Bucket 10: 2013-09-08 22:35:16 to 2014-04-28 10:49:19.
    # Min line num: Int=1, Str=1.
    # Max line num: Int=52949998, Str=9999987

    # First verify original folder is consistent (have all buckets for all folders)
    original_folder = Path(args.original_folder)
    assert original_folder.exists(), "Original folder does not exist"
    
    concept_group_dict_path = original_folder / 'concept_group_dict.json'
    # original_bucket_dict = load_json("/data3/zhiqiul/yfcc100m_all_new_sep_21/images_minbyte_10_valid_uploaded_date_minedge_120_maxratio_2.0/bucket_by_clear_10_time.json")
    # print(f"For original test folder:")
    # for b_idx in original_bucket_dict:
    #     min_line_num_str = None
    #     min_line_num_int = None
    #     max_line_num_str = None
    #     max_line_num_int = None
    #     min_date = original_bucket_dict[b_idx]['min_date']
    #     max_date = original_bucket_dict[b_idx]['max_date']
    #     all_metadata = original_bucket_dict[b_idx]['all_metadata']
    #     for meta in all_metadata:
    #         line_num_int = int(meta['LINE_NUM'])
    #         line_num_str = meta['LINE_NUM']
    #         if min_line_num_int == None or min_line_num_str == None:
    #             min_line_num_int = line_num_int
    #             min_line_num_str = line_num_str
    #         if max_line_num_int == None or max_line_num_str == None:
    #             max_line_num_int = line_num_int
    #             max_line_num_str = line_num_str
    #         if min_line_num_int > line_num_int:
    #             min_line_num_int = line_num_int
    #         if min_line_num_str > line_num_str:
    #             min_line_num_str = line_num_str
    #         if max_line_num_int < line_num_int:
    #             max_line_num_int = line_num_int
    #         if max_line_num_str < line_num_str:
    #             max_line_num_str = line_num_str
        
    #     print(f"Bucket {b_idx}: {min_date} to {max_date}. ")
    #     print(f"Min line num: Int={min_line_num_int}, Str={min_line_num_str}.")
    #     print(f"Max line num: Int={max_line_num_int}, Str={max_line_num_str}.")

    # import pdb; pdb.set_trace()
    MIN_LINE_NUM = 10060000
    class_names_path = original_folder / 'class_names.txt'
    filelists_json_path = original_folder / 'filelists.json'
    filelists_path = original_folder / 'filelists'
    labeled_images_path = original_folder / 'labeled_images'
    labeled_metadata_json_path = original_folder / 'labeled_metadata.json'
    labeled_metadata_path = original_folder / 'labeled_metadata'
    public_folder_labeled_metadata_json_path = Path(args.public_folder) / 'labeled_metadata.json'

    original_filelists = load_json(filelists_json_path)
    original_labeled_metadata = load_json(labeled_metadata_json_path)
    public_labeled_metadata = load_json(public_folder_labeled_metadata_json_path)
    assert sorted(list(original_filelists.keys())) == sorted(list(original_labeled_metadata.keys()))
    bucket_indices = sorted(list(original_filelists.keys()), key=lambda idx: int(idx))
    
    # assert all buckets exists in cleaned folder
    cleaned_folder = Path(args.cleaned_folder)
    assert cleaned_folder.exists(), "Cleaned folder does not exist"
    
    label_mapper = Path(args.label_mapper)
    if not label_mapper.exists():
        print(f"Please specify a label map at {label_mapper}")
        print(f"Here are all user-named labels for reference: ")
        print(retrieve_cleaned_folder_labels(cleaned_folder, bucket_indices))
        exit(0)
    else:
        label_mapper_dict = load_json(label_mapper)
    
    cleaned_dict = retrieve_cleaned_folder_information(cleaned_folder, bucket_indices, label_mapper_dict)

    new_folder = Path(args.new_folder)
    new_folder.mkdir(exist_ok=True)
    
    # sorted_prompts = class_names_path.read_text().split("\n")
    new_filelists_json_path = new_folder / 'filelists.json'
    new_filelists_path = new_folder / 'filelists'
    new_filelists_path.mkdir(exist_ok=True)

    new_labeled_images_path = new_folder / 'labeled_images'
    new_labeled_images_path.mkdir(exist_ok=True)

    new_labeled_metadata_json_path = new_folder / 'labeled_metadata.json'
    new_labeled_metadata_path = new_folder / 'labeled_metadata'
    new_labeled_metadata_path.mkdir(exist_ok=True)

    new_concept_group_dict_path = new_folder / 'concept_group_dict.json'
    shutil.copy(concept_group_dict_path, new_concept_group_dict_path)
    
    new_class_names_path = new_folder / "class_names.txt"
    new_class_names_str = "\n".join(cleaned_dict['sorted_prompts'])
    if new_class_names_path.exists():
        old_class_names_str = new_class_names_path.read_text()
        if not new_class_names_str == old_class_names_str:
            raise ValueError(f"Old class names do not match with current")
    else:
        with open(new_class_names_path, 'w+') as f:
            f.write(new_class_names_str)
    
    labeled_metadata = load_json(labeled_metadata_json_path)
    
    new_labeled_metadata_dict = {}
    new_filelists_dict = {}
    for b_idx in bucket_indices:
        new_labeled_metadata_path_i = new_labeled_metadata_path / b_idx
        new_labeled_metadata_path_i.mkdir(exist_ok=True)
        
        new_labeled_metadata_dict[b_idx] = {}

        labeled_images_path_i = labeled_images_path / b_idx
        assert labeled_images_path_i.exists()
        new_labeled_images_path_i = new_labeled_images_path / b_idx
        new_labeled_images_path_i.mkdir(exist_ok=True)
        
        new_filelists_path_i = new_filelists_path / (b_idx + ".txt")
        new_filelists_dict[b_idx] = str(Path('filelists') / (b_idx + ".txt"))
        
        new_filelist_strs_list_i = []

        original_metadata_count = 0
        original_metadata_count_after_line = 0
        cleaned_metadata_count = 0
        cleaned_metadata_count_after_line = 0

        for label in cleaned_dict['cleaned_images'][b_idx]:
            label_index = cleaned_dict['sorted_prompts'].index(label)
            new_labeled_images_path_i_label = new_labeled_images_path_i / label
            new_labeled_images_path_i_label.mkdir(exist_ok=True)

            new_labeled_metadata_path_i_label = new_labeled_metadata_path_i / (label + ".json")
            new_labeled_metadata_dict[b_idx][label] = str(Path('labeled_metadata') / b_idx / (label + ".json"))
            new_labeled_metadata_i_label = {} # key is flickr ID (str), value is metadata dict for this image
            
            labeled_metadata_i_label_path = labeled_metadata[b_idx][label] # key is flickr ID (str), value is metadata dict for this image
            labeled_metadata_i_label = load_json(original_folder / labeled_metadata_i_label_path)

            ID_list = cleaned_dict['cleaned_images'][b_idx][label]
            original_metadata_count += len(list(labeled_metadata_i_label.keys()))
            original_metadata_count_after_line += len([k for k in list(labeled_metadata_i_label.keys()) if int(labeled_metadata_i_label[k]['LINE_NUM']) > MIN_LINE_NUM])
            cleaned_metadata_count += len(ID_list)
            curr_cleaned_after_line = [ID for ID in ID_list if int(labeled_metadata_i_label[ID]['LINE_NUM']) > MIN_LINE_NUM]
            curr_cleaned_count_after_line = len(curr_cleaned_after_line)
            cleaned_metadata_count_after_line += curr_cleaned_count_after_line
            if curr_cleaned_count_after_line < args.num_imgs:
                print(f"{curr_cleaned_count_after_line} cleaned images for {b_idx} {label}")
                public_labeled_metadata_i_label = load_json(Path(args.public_folder) / public_labeled_metadata[b_idx][label])
                curr_cleaned_before_line = [ID for ID in ID_list if int(labeled_metadata_i_label[ID]['LINE_NUM']) < MIN_LINE_NUM]
                curr_cleaned_before_line_line_num = [labeled_metadata_i_label[ID]['LINE_NUM'] for ID in curr_cleaned_before_line]
                print(max(curr_cleaned_before_line_line_num))
                
                while len(curr_cleaned_after_line) < args.num_imgs:
                    ID = curr_cleaned_before_line.pop()
                    if ID not in public_labeled_metadata_i_label:
                        curr_cleaned_after_line.append(ID)
                        print(f"Add {ID} with line num {labeled_metadata_i_label[ID]['LINE_NUM']}")
            curr_cleaned_after_line = curr_cleaned_after_line[:args.num_imgs]
            print(f"Saving {len(curr_cleaned_after_line)} images with {b_idx} {label}")
            for ID in curr_cleaned_after_line:
                if not ID in labeled_metadata_i_label:
                    raise ValueError(f"{ID} not in original metadata dict")
                meta = labeled_metadata_i_label[ID]
                if int(meta['LINE_NUM']) > MIN_LINE_NUM:
                    cleaned_metadata_count_after_line += 1
                original_path = original_folder / meta['IMG_PATH']
                if not original_path.exists():
                    raise FileNotFoundError(f"{original_path} not found")
                assert ID == meta['ID']
                EXT = meta['EXT']
                img_name = f"{ID}.{EXT}"
                transfer_path = new_labeled_images_path_i_label / img_name
                shutil.copy(original_path, transfer_path)
                meta['IMG_DIR'] = str(new_folder)
                meta['IMG_PATH'] = str(Path("labeled_images") / b_idx / label / img_name)
                new_labeled_metadata_i_label[ID] = meta
                new_filelist_strs_list_i.append(f"{meta['IMG_PATH']} {label_index}")
        
            save_as_json(new_labeled_metadata_path_i_label, new_labeled_metadata_i_label)
        filelist_str = "\n".join(new_filelist_strs_list_i)
        with open(new_filelists_path_i, "w+") as f:
            f.write(filelist_str)
            
    save_as_json(new_filelists_json_path, new_filelists_dict)
    save_as_json(new_labeled_metadata_json_path, new_labeled_metadata_dict)
    
    exit(0)
    # optional folders. If exist, then copy all of them
    all_images_path = original_folder / 'all_images'
    all_metadata_path = original_folder / 'all_metadata'
    
    new_all_images_path = new_folder / 'all_images'
    new_all_metadata_path = new_folder / 'all_metadata'
    new_all_metadata_json_path = new_folder / 'all_metadata.json'

    if all_images_path.exists():
        shutil.copytree(all_images_path, new_all_images_path)
    
    if all_metadata_path.exists():
        new_all_metadata_dict = {}
        new_all_metadata_path.mkdir(exist_ok=True)
        for b_idx in bucket_indices:
            all_metadata_path_i = all_metadata_path / (b_idx + ".json")
            all_metadata_i = load_json(all_metadata_path_i) # key is flickr ID, value is metadata dict

            new_all_metadata_path_i = new_all_metadata_path / (b_idx + ".json")
            new_all_metadata_dict[b_idx] = str(Path('all_metadata') / (b_idx + ".json"))
            new_all_metadata_i = {} # key is flickr ID, value is metadata dict

            for ID in all_metadata_i:
                meta = all_metadata_i[ID]
                meta['IMG_DIR'] = str(new_folder)
                new_all_metadata_i[ID] = meta

            save_as_json(new_all_metadata_path_i, new_all_metadata_i)
        save_as_json(new_all_metadata_json_path, new_all_metadata_dict)
    