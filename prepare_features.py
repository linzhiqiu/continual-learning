from utils import load_json, save_as_json
import argparse
from tqdm import tqdm
import torch
import os
from pathlib import Path
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.datasets.folder import default_loader
import shutil

device = "cuda" if torch.cuda.is_available() else "cpu"

argparser = argparse.ArgumentParser()
argparser.add_argument("--folder_path",
                       default='/data3/zhiqiul/clear_datasets/CLEAR10-TEST',
                       help="The folder generated by prepare_concepts.py (containing labeled_metadata.json, etc.)")
argparser.add_argument("--name",
                       default='default', type=str,
                       help="Please name this model. Features will be saved under {args.folder_path}/features/{args.name}")
argparser.add_argument("--state_dict_path",
                       type=str,
                       help="Specify the path of this pre-trained model state_dict.")
argparser.add_argument("--arch",
                       type=str, default='resnet50',
                       help="Specify the architecture here.")

class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, samples, transform):
        self.samples = samples
        self.transform = transform
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self,index):
        path, ID = self.samples[index]
        sample = default_loader(path)
        sample = self.transform(sample)
        return sample, ID

def extract_features_dict(model, transform, paths_and_IDs_list):
    model.to(device)
    loader = torch.utils.data.DataLoader(
        SimpleDataset(paths_and_IDs_list,transform),
        shuffle=False,
        batch_size=10,
        num_workers=4,
    )
    features_dict = {}
    with torch.no_grad():
        for inputs, IDs in tqdm(loader):
            inputs = inputs.cuda()
            outputs = model(inputs)
            for output, ID in zip(outputs, IDs):
                features_dict[ID] = output.cpu()
    return features_dict


def extract_features(dataset_dict_i, feature_name, feature_extractor, batch_size=64):
    # batch size here is simply used to extract the features, not for final training purposes
    all_query = sorted(list(dataset_dict_i.keys()))
    features_dict_i = {}
    if feature_extractor == None:
        for k_name in dataset_dict_i[all_query[0]]:
            items = []
            for q_idx, query in enumerate(all_query):
                items += [(f, q_idx)
                          for f in dataset_dict_i[query][k_name][feature_name]]
            features_dict_i[k_name] = items
    else:
        feature_extractor = feature_extractor.cuda()
        for k_name in dataset_dict_i[all_query[0]]:
            items = []
            for q_idx, query in enumerate(all_query):
                items += [(f, q_idx)
                          for f in dataset_dict_i[query][k_name]['metadata']]
            loader = training_utils.make_image_loader(items, batch_size, shuffle=False, fixed_crop=True)
            extracted_items = []
            for inputs, labels in tqdm(loader):
                inputs = inputs.cuda()
                outputs = feature_extractor(inputs)
                for output, label in zip(outputs, labels):
                    extracted_items.append((output.cpu(), int(label)))
                    assert int(label) == items[len(extracted_items)-1][1]
            features_dict_i[k_name] = extracted_items
    return features_dict_i


class MLP(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)

    def forward(self, x):
        hidden = self.fc1(x)
        relu = self.relu(hidden)
        output = self.fc2(relu)
        return output

def make_feature_extractor(train_mode):
    assert train_mode in TRAIN_MODES_CATEGORY.keys()
    feature_type = TRAIN_MODES_CATEGORY[train_mode].feature_type
    if feature_type == 'clip':
        feature_extractor = None
        feature_name = 'clip_features'
    elif feature_type == 'image':
        feature_extractor = None
        feature_name = 'metadata'
    elif feature_type == 'cnn_feature':
        feature_extractor = make_cnn_model(TRAIN_MODES_CATEGORY[train_mode].pretrained_weight, output_size=None, train_mode='freeze')
        feature_name = 'metadata'
    return feature_name, feature_extractor

def make_cnn_model(pretrained_weight, output_size=1000, train_mode='finetune'):
    print(f"Using ResNet 50 (frozen feature extractor)")
    pretrained = False
    selfsupervised = False
    if pretrained_weight == 'imgnet':
        pretrained = True
    elif pretrained_weight == 'moco_imgnet':
        selfsupervised = 'moco_v2'
    elif pretrained_weight == 'byol_imgnet':
        selfsupervised = 'byol'
    elif pretrained_weight == "moco_yfcc_feb18_gpu_8_bucket_0":
        selfsupervised = "moco_v2_yfcc_feb18_bucket_0_gpu_8"
    elif pretrained_weight == None:
        pass
    else:
        raise NotImplementedError()
    model = training_utils.make_model(
        'resnet50',
        pretrained,
        selfsupervised,
        train_mode=train_mode,
        output_size=output_size
    )
    if train_mode == 'freeze':
        model.eval()
    return model

def get_input_size(train_mode):
    feature_type = TRAIN_MODES_CATEGORY[train_mode].feature_type
    if feature_type == 'clip':
        input_size = 1024
    elif feature_type == 'cnn_feature':
        input_size = 2048
    elif feature_type == 'image':
        input_size = None
    else:
        raise NotImplementedError()
    return input_size

def make_model(train_mode, output_size=1000):
    network_type = TRAIN_MODES_CATEGORY[train_mode].network_type
    input_size = get_input_size(train_mode)
    if network_type == 'mlp' or network_type == 'mlp_tuned':
        print(f"Using a mlp network with input size {input_size}")
        mlp = MLP(input_size, 2048, output_size)
        return mlp
    elif network_type in ['linear' , 'linear_tuned', 'linear_tuned_2', 'linear_batch_8', 'linear_tuned_batch_8']:
        print(f"Using a single linear layer")
        fc = torch.nn.Linear(input_size, output_size)
        # import pdb; pdb.set_trace()
        # fc.weight.data.normal_(mean=0.0, std=0.01)
        # fc.bias.data.zero_()
        return fc
    elif network_type in ['cnn', 'cnn_lower_lr']:
        return make_cnn_model(TRAIN_MODES_CATEGORY[train_mode].pretrained_weight,
                              output_size=output_size,
                              train_mode='finetune')

if __name__ == '__main__':
    args = argparser.parse_args()

    folder_path = Path(args.folder_path)
    assert folder_path.exists(), f"{folder_path} does not exist"

    class_names_path = folder_path / 'class_names.txt'
    class_names_str = class_names_path.read_text()
    sorted_prompts = class_names_str.split("\n")

    labeled_images_path = folder_path / 'labeled_images'
    assert labeled_images_path.exists()

    labeled_metadata_json_path = folder_path / 'labeled_metadata.json'
    labeled_metadata = load_json(labeled_metadata_json_path)
    bucket_indices = sorted(list(labeled_metadata.keys()), key = lambda x : int(x))

    features_path = folder_path / 'features'
    features_path.mkdir(exist_ok=True)
    features_name_path = features_path / args.name
    features_name_path.mkdir(exist_ok=True)
    features_dict_path = features_name_path / 'features.json'
    state_dict_save_path = features_name_path / f'state_dict.pth.tar'
    features_dict = {} # features_dict[bucket_index] = path to features dictionary
    
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
    test_transform = transforms.Compose([
        transforms.Resize(224),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize,
    ])
    
    print("=> using arch '{}'".format(args.arch))
    model = models.__dict__[args.arch](pretrained=False)
    
    shutil.copy(args.state_dict_path, state_dict_save_path)
    print(f"Copied {args.state_dict_path} to {state_dict_save_path}")
    state_dict = torch.load(args.state_dict_path)
    model.fc = torch.nn.Identity()
    model.load_state_dict(state_dict)
    for p in model.parameters():
        p.requires_grad = False
    model.eval()

    for b_idx in bucket_indices:
        print(f"Working on {b_idx}")
        features_path_i = features_name_path / b_idx
        features_path_i.mkdir(exist_ok=True)
        features_dict[b_idx] = {} # key is label, value is dict
        
        for label in labeled_metadata[b_idx]:
            features_path_i_label = features_path_i / (label + ".pth")
            features_dict[b_idx][label] = str(Path("features") / b_idx / (label + ".pth"))
            paths_i_label = [] # list of 2 element tuple. first element is image path, second element is ID

            labeled_metadata_path_i_label = labeled_metadata[b_idx][label]
            labeled_metadata_i_label = load_json(folder_path / labeled_metadata_path_i_label)
            for ID in labeled_metadata_i_label:
                path = Path(args.folder_path) / labeled_metadata_i_label[ID]['IMG_PATH']
                paths_i_label.append([path, ID])
            
            features_i_label = extract_features_dict(model, test_transform, paths_i_label) # key is flickr ID (str), value is tensor feature
            torch.save(features_i_label, features_path_i_label)
    save_as_json(features_dict_path, features_dict)
    print(f"Feature information saved to {features_dict_path}")
    